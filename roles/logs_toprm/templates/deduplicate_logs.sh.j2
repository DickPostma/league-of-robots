#!/bin/bash
set -eEuo pipefail
_log_tag="logstoprm"
# We store all the extra information into the system logs, like civilized people
trap 'logger -t "${_log_tag}" "${$} Command (${BASH_COMMAND}) failed on line number (${LINENO})"' ERR

# This script checkes all the files for the duplicates
#  - first it collects the list of all the files
#  - then iterates through each file and looks for the duplicate
#  - duplicates are log files of similar path and same
#      ./ [earl*] / [machine] / compressed_files / [log type]-[date]
#  - checks if the files are the same
#    - if so, then update the timestamp
#    - if not, then merge both files into one, keep only unique lines
#  - finally make one file a hardlink to another (to reduce storage),
#    but keep each file, or rsync would recreate them at the next run

{% raw %}
_base_dir="{% endraw %}{{ logs_toprm_destination }}{% raw %}"
cd "${_base_dir}"
# find all files that are not already hardlinked (=skip ones that are already done)
# note: '-links 1' returns one or more links, therefore '<2' is needed
_all_files="$(find ./ -type f -links -2)"
_count=0

logger -t "${_log_tag}" "${0}  Starting to deduplicate files in the ${_base_dir}"
while IFS= read -r _line; do
    _earl="$(echo "${_line}" | awk -F'/' '{print $2}')"
    _machine="$(echo "${_line}" | awk -F'/' '{print $3}')"
    _file="$(echo "${_line}" | awk -F'/' '{print $6}')"
    _replica=$(awk -F '/' -v aearl="${_earl}" -v amachine="${_machine}" -v afile="${_file}" '$2 != aearl && $3 == amachine && $6 == afile { print $0 }' <<< "${_all_files}")
    if [[ -n "${_replica}" ]]; then
        if ! diff ${_line} ${_replica} > /dev/null ; then
            # merge both files into one, keep only unique lines from each
            zcat ${_line} ${_replica} 2>/dev/null | sort -u | gzip > "${_line}.tmp" && sync
            mv -f "${_line}.tmp" "${_line}"
        else
            # files are the same, then only change the timestamp to right now
            # so that rsync will never overwrite it anymore
            touch ${_line}
        fi
        # now hard link them together - it does not take extra storage and it works with
        # rsync -au which does not update if newer exist
        ln -f ${_line} ${_replica}
        _count=$(( _count + 1))
    fi
done <<< "${_all_files}"
logger -t "${_log_tag}" "${0}  Successfully finished deduplicating. Deduplicated ${_count} files."
{% endraw %}

