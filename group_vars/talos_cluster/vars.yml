---
slurm_cluster_name: 'talos'
slurm_cluster_domain: 'hpc.rug.nl'
stack_name: "{{ slurm_cluster_name }}_cluster"  # stack_name must match the name of the folder that contains this vars.yml file.
stack_prefix: 'tl'
slurm_version: '20.11.8-1.el7.umcg'
slurm_allow_jobs_to_span_nodes: true
slurm_partitions:
  - name: regular  # Must be in sync with group listed in Ansible inventory.
    default: yes
    nodes: "{{ stack_prefix }}-vcompute[01-03]"  # Must be in sync with Ansible hostnames listed in inventory.
    max_nodes_per_job: "{% if slurm_allow_jobs_to_span_nodes is defined and slurm_allow_jobs_to_span_nodes is true %}{{ groups['regular']|list|length }}{% else %}1{% endif %}"
    max_cores_per_node: "{{ groups['regular'] | map('extract', hostvars, 'slurm_max_cpus_per_node') | first }}"
    max_mem_per_node: "{{ groups['regular'] | map('extract', hostvars, 'slurm_max_mem_per_node') | first }}"
    local_disk: "{{ groups['regular'] | map('extract', hostvars, 'slurm_local_disk') | first | default(0, true) }}"
    features: "{{ groups['regular'] | map('extract', hostvars, 'slurm_features') | first | default('none') }}"
    extra_options: 'TRESBillingWeights="CPU=1.0,Mem=0.5G" DenyQos=ds-short,ds-medium,ds-long'
  - name: user_interface  # Must be in sync with group listed in Ansible inventory.
    default: no
    nodes: "{{ slurm_cluster_name }}"  # Must be in sync with Ansible hostnames listed in inventory.
    max_nodes_per_job: 1
    max_cores_per_node: 1
    max_mem_per_node: 1024
    local_disk: "{{ groups['user_interface'] | map('extract', hostvars, 'slurm_local_disk') | first | default(0, true) }}"
    features: "{{ groups['user_interface'] | map('extract', hostvars, 'slurm_features') | first | default('none') }}"
    extra_options: 'TRESBillingWeights="CPU=1.0,Mem=1.0G" AllowQos=ds-short,ds-medium,ds-long'
repo_manager: 'pulp'
mailhub: '172.23.34.34'
rewrite_domain: "{{ stack_prefix }}-sai{% if slurm_cluster_domain | length %}.{{ slurm_cluster_domain }}{% endif %}"
figlet_font: 'ogre'
motd: |
      =========================================================
      !!! WARNING: {{ slurm_cluster_name | capitalize }} is in beta testing
      =========================================================
          This cluster may be redeployed from scratch, which
          may result in complete data loss of home dirs 
          and tmp0* group folders: You have been warned!!!
          This does not affect prm0* group folders,
          which are on a different (production) storage system.
      =========================================================
additional_etc_hosts:
  - group: all
    hosts:
      - docs
      - docs_on_merlin
  - group: boxy_cluster
    hosts:
      - boxy-management
      - boxy-storage
  - group: calculon_cluster
    hosts:
      - calculon-management
      - calculon-storage
ssh_host_signer_ca_private_key: "{{ ssh_host_signer_ca_keypair_dir }}/umcg-hpc-development-ca"
use_ldap: yes
create_ldap: no
use_sssd: yes
ldap_domains:
  grunn:
    uri: ldaps://172.23.40.249
    search_base: ou=umcg,o=asds
    schema: rfc2307
    min_id: 50100000
    max_id: 55999999
    user_object_class: posixAccount
    user_name: uid
    user_ssh_public_key: sshPublicKey
    user_member_of: groupMembership
    group_member: memberUid
    group_object_class: groupofnames
    group_quota_soft_limit_template: ruggroupumcgquotaLFSsoft
    group_quota_hard_limit_template: ruggroupumcgquotaLFS
nameservers: [
  '/gcc-storage001.stor.hpc.local/172.23.40.244',  # Local DNS lookups for shared storage.
  '8.8.4.4',  # Google DNS.
  '8.8.8.8',  # Google DNS.
]
local_admin_groups:
  - 'admin'
  - 'docker'
local_admin_users:
  - 'alex2'
  - 'egon'
  - 'ger'
  - 'gerben'
  - 'henkjan'
  - 'kim'
  - 'marieke'
  - 'marlies'
  - 'marloes'
  - 'morris'
  - 'pieter'
  - 'robin'
  - 'sandi'
  - 'wim'
data_transfer_only_group: 'umcg-sftp-only'
envsync_user: 'umcg-envsync'
envsync_group: 'umcg-depad'
functional_admin_group: 'umcg-funad'
hpc_env_prefix: '/apps'
regular_groups:
  - "{{ data_transfer_only_group }}"
  - "{{ envsync_group }}"
  - "{{ functional_admin_group }}"
  - 'umcg-atd'
  - 'umcg-endocrinology'
  - 'umcg-gcc'
  - 'umcg-lifelines'
  - 'umcg-sysops'
regular_users:
  - user: "{{ envsync_user }}"
    groups: ["{{ envsync_group }}"]
  - user: 'umcg-atd-ateambot'
    groups: ['umcg-atd']
    sudoers: '%umcg-atd'
  - user: 'umcg-atd-dm'
    groups: ['umcg-atd']
    sudoers: '%umcg-atd'
  - user: 'umcg-cineca-dm'
    groups: ['umcg-cineca']
    sudoers: '%umcg-cineca-dms'
  - user: 'umcg-endocrinology-dm'
    groups: ['umcg-endocrinology']
    sudoers: '%umcg-endocrinology-dms'
  - user: 'umcg-gcc-dm'
    groups: ['umcg-gcc']
    sudoers: '%umcg-gcc'
  - user: 'umcg-lifelines-dm'
    groups: ['umcg-lifelines']
    sudoers: '%umcg-lifelines-dms'
  - user: 'umcg-sysops-dm'
    groups: ['umcg-sysops']
    sudoers: '%umcg-sysops'
#
# Shared storage related variables
#
pfs_mounts:
  - pfs: umcgst11
    source: 'gcc-storage001.stor.hpc.local:/ifs/rekencluster'
    type: nfs4
    rw_options: 'defaults,_netdev,vers=4.0,noatime,nodiratime'
    ro_options: 'defaults,_netdev,vers=4.0,noatime,nodiratime,ro'
    machines: "{{ groups['sys_admin_interface'] }}"
lustre_quota_type: group
lfs_mounts:
  - lfs: home
    pfs: umcgst11
    rw_machines: "{{ groups['cluster'] }}"
  - lfs: tmp08
    pfs: umcgst11
    groups:
      - name: umcg-atd
      - name: umcg-cineca
      - name: umcg-endocrinology
      - name: umcg-gcc
      - name: umcg-lifelines
        mode: '2750'
      - name: umcg-sysops
    rw_machines: "{{ groups['user_interface'] + groups['compute_vm'] }}"
  - lfs: rsc08
    pfs: umcgst11
    groups:
      - name: umcg-atd
      - name: umcg-lifelines
      - name: umcg-sysops
    ro_machines: "{{ groups['compute_vm'] }}"
    rw_machines: "{{ groups['user_interface'] }}"
  - lfs: prm08
    pfs: umcgst11
    groups:
      - name: umcg-atd
      - name: umcg-cineca
      - name: umcg-gcc
      - name: umcg-lifelines
      - name: umcg-solve-rd
      - name: umcg-sysops
    rw_machines: "{{ groups['user_interface'] }}"
  - lfs: env08
    pfs: umcgst11
    ro_machines: "{{ groups['compute_vm'] + groups['user_interface'] }}"
    rw_machines: "{{ groups['deploy_admin_interface'] }}"
ega_fuse_client_mounts:
  cineca: '/groups/umcg-cineca/prm08/ega-fuse-client'
  solve_rd: '/groups/umcg-solve-rd/prm08/ega-fuse-client'
ega_fuse_client_java_home: '/etc/alternatives/jre_11_openjdk'
iptables_allow_icmp_inbound:
  - 'umcg_net1'
  - 'umcg_net2'
  - 'umcg_net3'
  - 'rug_bwp_net'
  - 'rug_operator'
  - 'rug_gcc_cloud_net'
  - 'foyer'
  - 'boxy'
  - 'bender'
  - 'lobby'
  - 'calculon'
  - 'flexo'
  - 'gate'
  - 'zinc_finger'
  - 'coenzyme'
  - 'passage'
  - 'leucine_zipper'
  - 'chaperone'
  - 'airlock'
  - 'jenkins1'
  - 'jenkins2'
iptables_allow_ssh_inbound:
  - 'umcg_net1'
  - 'umcg_net2'
  - 'umcg_net3'
  - 'rug_bwp_net'
  - 'rug_operator'
  - 'foyer'
  - 'boxy'
  - 'bender'
  - 'lobby'
  - 'calculon'
  - 'flexo'
  - 'gate'
  - 'zinc_finger'
  - 'coenzyme'
  - 'passage'
  - 'leucine_zipper'
  - 'chaperone'
  - 'airlock'
  - 'jenkins1'
  - 'jenkins2'
iptables_allow_ssh_outbound:
  - 'foyer'
  - 'boxy'
  - 'bender'
  - 'lobby'
  - 'calculon'
  - 'flexo'
  - 'gate'
  - 'peregrine'
  - 'gattaca01'
  - 'gattaca02'
  - 'cher_ami'
  - 'eriba_ds'
  - 'molgenis_downloads'
  - 'airlock'
  - 'surfsara_grid_ui'
  - 'lumc_shark_ui'
  - 'cnag_sftp'
  - 'erasmus_mc_net'
  - 'rug_f5_net'
  - 'sanger_sftp'
iptables_allow_ebi_mysql_outbound:
  - 'ebi_sanger_net1'
  - 'ebi_sanger_net2'
iptables_allow_ftp_outbound:
  - 'ebi_sanger_net1'
  - 'ebi_sanger_net2'
  - 'broad_ftp'
  - 'ncbi_net1'
  - 'ncbi_net2'
iptables_allow_aspera_outbound:
  - 'ebi_sanger_net1'
  - 'ebi_sanger_net2'
  - 'broad_aspera_1'
  - 'broad_aspera_2'
  - 'broad_aspera_3'
  - 'broad_aspera_4'
  - 'broad_aspera_5'
  - 'broad_aspera_6'
  - 'broad_aspera_7'
  - 'broad_aspera_8'
  - 'broad_aspera_9'
  - 'ncbi_net1'
  - 'ncbi_net2'
iptables_allow_globus_outbound:
  - 'sanger_globus'
...
